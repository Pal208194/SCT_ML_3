# Hand Gesture Recognition System
# Complete implementation for detecting, classifying, and using hand gestures for HCI

import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import os
import pickle
import time

class GestureRecognitionSystem:
    def __init__(self):
        # Initialize MediaPipe hands module
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # Gesture classes
        self.gesture_classes = ['thumbs_up', 'thumbs_down', 'peace', 'fist', 'open_palm', 'pointing']
        
        # Path configurations
        self.model_path = 'gesture_model.h5'
        self.data_dir = 'gesture_data'
        
        # Initialize model
        if os.path.exists(self.model_path):
            self.model = load_model(self.model_path)
            print("Model loaded from disk")
        else:
            self.model = self._create_model()
            print("New model created")
    
    def _create_model(self):
        """Create a CNN model for gesture classification"""
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Conv2D(128, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(len(self.gesture_classes), activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def collect_training_data(self, num_samples=50):
        """Collect training data for each gesture class"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            
        cap = cv2.VideoCapture(0)
        
        for gesture_class in self.gesture_classes:
            gesture_dir = os.path.join(self.data_dir, gesture_class)
            if not os.path.exists(gesture_dir):
                os.makedirs(gesture_dir)
            
            print(f"Prepare to record {gesture_class} gesture.")
            print("Press 's' to start recording")
            
            while True:
                ret, frame = cap.read()
                cv2.putText(frame, f"Ready to record {gesture_class}?", (50, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.imshow('Data Collection', frame)
                
                if cv2.waitKey(1) & 0xFF == ord('s'):
                    break
            
            count = 0
            print(f"Recording {gesture_class}. Make the gesture and hold...")
            
            while count < num_samples:
                ret, frame = cap.read()
                if not ret:
                    continue
                
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.hands.process(rgb_frame)
                
                if results.multi_hand_landmarks:
                    for hand_landmarks in results.multi_hand_landmarks:
                        self.mp_drawing.draw_landmarks(
                            frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                    
                    # Save the frame
                    timestamp = int(time.time() * 1000)
                    filename = f"{gesture_class}_{count}_{timestamp}.jpg"
                    filepath = os.path.join(gesture_dir, filename)
                    
                    # Save processed hand image
                    h, w, _ = frame.shape
                    x_min, y_min = w, h
                    x_max, y_max = 0, 0
                    
                    # Find hand bounding box
                    for landmark in hand_landmarks.landmark:
                        x, y = int(landmark.x * w), int(landmark.y * h)
                        x_min = min(x_min, x)
                        y_min = min(y_min, y)
                        x_max = max(x_max, x)
                        y_max = max(y_max, y)
                    
                    # Add padding
                    padding = 20
                    x_min = max(0, x_min - padding)
                    y_min = max(0, y_min - padding)
                    x_max = min(w, x_max + padding)
                    y_max = min(h, y_max + padding)
                    
                    # Crop hand region
                    hand_img = frame[y_min:y_max, x_min:x_max]
                    if hand_img.size > 0:  # Ensure valid crop
                        hand_img = cv2.resize(hand_img, (224, 224))
                        cv2.imwrite(filepath, hand_img)
                        count += 1
                        print(f"Saved {count}/{num_samples} for {gesture_class}")
                
                cv2.putText(frame, f"Recording {gesture_class}: {count}/{num_samples}", 
                           (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.imshow('Data Collection', frame)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                    
        cap.release()
        cv2.destroyAllWindows()
        print("Data collection completed!")
    
    def prepare_dataset(self):
        """Prepare dataset for training"""
        X = []
        y = []
        
        for idx, gesture_class in enumerate(self.gesture_classes):
            gesture_dir = os.path.join(self.data_dir, gesture_class)
            if not os.path.exists(gesture_dir):
                print(f"No data directory found for {gesture_class}")
                continue
                
            for image_file in os.listdir(gesture_dir):
                image_path = os.path.join(gesture_dir, image_file)
                try:
                    img = cv2.imread(image_path)
                    img = cv2.resize(img, (224, 224))
                    img = img / 255.0  # Normalize
                    
                    X.append(img)
                    # One-hot encoding
                    label = [0] * len(self.gesture_classes)
                    label[idx] = 1
                    y.append(label)
                except Exception as e:
                    print(f"Error processing {image_path}: {e}")
        
        return np.array(X), np.array(y)
    
    def train(self, epochs=20, batch_size=32):
        """Train the gesture recognition model"""
        X, y = self.prepare_dataset()
        
        if len(X) == 0:
            print("No training data available. Please collect data first.")
            return
            
        # Split into training and validation
        indices = np.random.permutation(len(X))
        train_size = int(0.8 * len(X))
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]
        
        X_train, y_train = X[train_indices], y[train_indices]
        X_val, y_val = X[val_indices], y[val_indices]
        
        print(f"Training on {len(X_train)} samples, validating on {len(X_val)} samples")
        
        # Train the model
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val)
        )
        
        # Save the model
        self.model.save(self.model_path)
        print(f"Model saved to {self.model_path}")
        
        return history
    
    def process_landmarks(self, frame, landmarks):
        """Process hand landmarks into a format suitable for classification"""
        h, w, _ = frame.shape
        # Extract hand region
        x_min, y_min = w, h
        x_max, y_max = 0, 0
        
        for landmark in landmarks.landmark:
            x, y = int(landmark.x * w), int(landmark.y * h)
            x_min = min(x_min, x)
            y_min = min(y_min, y)
            x_max = max(x_max, x)
            y_max = max(y_max, y)
        
        # Add padding
        padding = 20
        x_min = max(0, x_min - padding)
        y_min = max(0, y_min - padding)
        x_max = min(w, x_max + padding)
        y_max = min(h, y_max + padding)
        
        # Crop and resize hand region
        hand_img = frame[y_min:y_max, x_min:x_max]
        if hand_img.size == 0:  # Empty crop
            return None
            
        hand_img = cv2.resize(hand_img, (224, 224))
        hand_img = hand_img / 255.0  # Normalize
        
        return hand_img, (x_min, y_min, x_max, y_max)
    
    def predict_gesture(self, hand_img):
        """Predict gesture class from hand image"""
        if hand_img is None:
            return None, 0
            
        # Add batch dimension
        hand_img = np.expand_dims(hand_img, axis=0)
        
        # Make prediction
        prediction = self.model.predict(hand_img)[0]
        gesture_idx = np.argmax(prediction)
        confidence = prediction[gesture_idx]
        
        return self.gesture_classes[gesture_idx], confidence
    
    def run_detection(self):
        """Run real-time gesture detection"""
        cap = cv2.VideoCapture(0)
        prev_gesture = None
        gesture_counter = 0
        stable_threshold = 3  # Number of consistent frames to confirm a gesture
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                print("Failed to capture video")
                break
                
            # Convert to RGB for MediaPipe
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(rgb_frame)
            
            detected_gesture = None
            confidence = 0
            
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    # Draw landmarks
                    self.mp_drawing.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                    
                    # Process landmarks for classification
                    processed_img, bbox = self.process_landmarks(frame, hand_landmarks)
                    if processed_img is not None:
                        # Predict gesture
                        gesture, conf = self.predict_gesture(processed_img)
                        if conf > confidence:  # Take highest confidence if multiple hands
                            detected_gesture = gesture
                            confidence = conf
                            x_min, y_min, x_max, y_max = bbox
                            
                            # Draw bounding box
                            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
            
            # Stabilize gesture detection
            if detected_gesture == prev_gesture:
                gesture_counter += 1
            else:
                gesture_counter = 0
                
            prev_gesture = detected_gesture
            
            # Display stable gesture
            if gesture_counter >= stable_threshold and detected_gesture:
                cv2.putText(frame, f"{detected_gesture.upper()} ({confidence:.2f})", 
                           (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
                # Here you can trigger actions based on detected gestures
                self.execute_gesture_action(detected_gesture)
            
            cv2.imshow('Gesture Recognition', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
        cap.release()
        cv2.destroyAllWindows()
    
    def execute_gesture_action(self, gesture):
        """Execute actions based on detected gestures"""
        # This method can be expanded to integrate with various systems
        actions = {
            'thumbs_up': "Increase volume/Approve",
            'thumbs_down': "Decrease volume/Disapprove",
            'peace': "Take screenshot/Select item",
            'fist': "Stop current operation",
            'open_palm': "Pause/Resume operation",
            'pointing': "Navigate cursor/Select"
        }
        
        print(f"Action: {actions.get(gesture, 'No action defined')}")
        
        # Example: you could implement system controls here using pyautogui or similar
        # For example, to simulate volume up/down, cursor movement, etc.

# Deployment code
class GestureControlSystem:
    def __init__(self, gesture_recognizer):
        self.gesture_recognizer = gesture_recognizer
        self.controllers = {
            'media_controller': self.control_media,
            'pointer_controller': self.control_pointer,
            'system_controller': self.control_system
        }
        self.active_controller = 'system_controller'
    
    def control_media(self, gesture):
        """Media control actions"""
        if gesture == 'thumbs_up':
            print("ACTION: Volume Up")
            # Implement volume up functionality
        elif gesture == 'thumbs_down':
            print("ACTION: Volume Down")
            # Implement volume down functionality
        elif gesture == 'fist':
            print("ACTION: Pause/Play")
            # Implement pause/play functionality
        elif gesture == 'peace':
            print("ACTION: Next Track")
            # Implement next track functionality
    
    def control_pointer(self, gesture):
        """Mouse pointer control"""
        if gesture == 'pointing':
            print("ACTION: Move Cursor")
            # Implement cursor movement
        elif gesture == 'fist':
            print("ACTION: Left Click")
            # Implement left click
        elif gesture == 'peace':
            print("ACTION: Right Click")
            # Implement right click
    
    def control_system(self, gesture):
        """System control actions"""
        if gesture == 'open_palm':
            print("ACTION: Switch Application")
            # Implement app switching (Alt+Tab equivalent)
        elif gesture == 'thumbs_up':
            print("ACTION: Increase Brightness")
            # Implement brightness up
        elif gesture == 'thumbs_down':
            print("ACTION: Decrease Brightness")
            # Implement brightness down
    
    def switch_controller(self, controller_name):
        """Switch between different control modes"""
        if controller_name in self.controllers:
            self.active_controller = controller_name
            print(f"Switched to {controller_name}")
        else:
            print(f"Controller {controller_name} not found")
    
    def process_gesture(self, gesture):
        """Process detected gesture using active controller"""
        controller_function = self.controllers.get(self.active_controller)
        if controller_function:
            controller_function(gesture)

# Main deployment script
if __name__ == "__main__":
    print("Initializing Hand Gesture Recognition System...")
    
    # Step 1: Initialize the gesture recognition system
    gesture_system = GestureRecognitionSystem()
    
    # Step 2: Check if training data exists, otherwise collect it
    if not os.path.exists(gesture_system.data_dir) or \
       not any(os.listdir(gesture_system.data_dir)):
        print("No training data found. Collecting training data...")
        gesture_system.collect_training_data()
    
    # Step 3: Check if model exists, otherwise train it
    if not os.path.exists(gesture_system.model_path):
        print("No trained model found. Training model...")
        gesture_system.train()
    
    # Step 4: Initialize the control system
    control_system = GestureControlSystem(gesture_system)
    
    # Step 5: Run the detection system
    print("Starting gesture detection. Press 'q' to quit.")
    gesture_system.run_detection()
